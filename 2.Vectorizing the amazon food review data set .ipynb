{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do Vectorizing on Amazon food reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (568454, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading amazon food review data set\n",
    "data=pd.read_csv('Reviews.csv')\n",
    "print(\"shape\",data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 222 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5    363122\n",
       "4     80655\n",
       "1     52268\n",
       "3     42640\n",
       "2     29769\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 496 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Converting score coloumn to positive or negative review\n",
    "data['Score'] = data['Score'].apply(lambda x : 'pos' if x > 3 else 'neg')\n",
    "data.groupby('Score')['Summary'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Functions to save objects for later use and retireve it\n",
    "import pickle\n",
    "def savetofile(obj,filename):\n",
    "   pickle.dump(obj,open(filename,\"wb\"))\n",
    "def openfromfile(filename):\n",
    "   temp = pickle.load(open(filename,\"rb\"))\n",
    "   return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 467 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Loading the variable from file\n",
    "data_100000 = openfromfile(\"f_string.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1e+03 Âµs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>witti littl book make son laugh loud recit car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>rememb see show air televis year ago child sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>beetlejuic well written movi everyth excel act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>pos</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>beetlejuic excel funni movi keaton hilari wack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId               ProfileName  \\\n",
       "0  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
       "1  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "2  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "3  374359  B00004CI84  A344SMIA5JECGM           Vincent P. Ross   \n",
       "4  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator Score       Time  \\\n",
       "0                     0                       0   pos  939340800   \n",
       "1                     2                       2   pos  940809600   \n",
       "2                     0                       0   pos  944092800   \n",
       "3                     1                       2   pos  944438400   \n",
       "4                     0                       0   pos  946857600   \n",
       "\n",
       "                                             Summary  \\\n",
       "0                          EVERY book is educational   \n",
       "1  This whole series is great way to spend time w...   \n",
       "2                               Entertainingl Funny!   \n",
       "3                            A modern day fairy tale   \n",
       "4                                         FANTASTIC!   \n",
       "\n",
       "                                                Text  \\\n",
       "0  this witty little book makes my son laugh at l...   \n",
       "1  I can remember seeing the show when it aired o...   \n",
       "2  Beetlejuice is a well written movie ..... ever...   \n",
       "3  A twist of rumplestiskin captured on film, sta...   \n",
       "4  Beetlejuice is an excellent and funny movie. K...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  witti littl book make son laugh loud recit car...  \n",
       "1  rememb see show air televis year ago child sis...  \n",
       "2  beetlejuic well written movi everyth excel act...  \n",
       "3  twist rumplestiskin captur film star michael k...  \n",
       "4  beetlejuic excel funni movi keaton hilari wack...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data_100000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels = data_100000['Score']\n",
    "data_100000['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Making the index from 0 to end\n",
    "data_100000 = data_100000.reset_index(drop=True)\n",
    "data_100000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 76 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sorting the data based on time\n",
    "data_100000.sort_values('Time',inplace=True)\n",
    "data_100000 = data_100000.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#In the Score column, assigning positive as 1 and negative as 0\n",
    "def pos_neg(x):\n",
    "    if x == 'pos':\n",
    "        return 1\n",
    "    return 0\n",
    "data_100000['Score'] = data_100000['Score'].map(pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ax=plt.axes()\n",
    "sns.countplot(data_100000.Score,ax=ax)\n",
    "plt.title(\"Scores\")\n",
    "plt.show()\n",
    "data_100000['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(data_100000,\"prepocessed_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 530 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Loading the variable from file\n",
    "final_100000 = openfromfile(\"prepocessed_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>witti littl book make son laugh loud recit car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>rememb see show air televis year ago child sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>beetlejuic well written movi everyth excel act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>beetlejuic excel funni movi keaton hilari wack...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId               ProfileName  \\\n",
       "0  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
       "1  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "2  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "3  374359  B00004CI84  A344SMIA5JECGM           Vincent P. Ross   \n",
       "4  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "0                     0                       0      1  939340800   \n",
       "1                     2                       2      1  940809600   \n",
       "2                     0                       0      1  944092800   \n",
       "3                     1                       2      1  944438400   \n",
       "4                     0                       0      1  946857600   \n",
       "\n",
       "                                             Summary  \\\n",
       "0                          EVERY book is educational   \n",
       "1  This whole series is great way to spend time w...   \n",
       "2                               Entertainingl Funny!   \n",
       "3                            A modern day fairy tale   \n",
       "4                                         FANTASTIC!   \n",
       "\n",
       "                                                Text  \\\n",
       "0  this witty little book makes my son laugh at l...   \n",
       "1  I can remember seeing the show when it aired o...   \n",
       "2  Beetlejuice is a well written movie ..... ever...   \n",
       "3  A twist of rumplestiskin captured on film, sta...   \n",
       "4  Beetlejuice is an excellent and funny movie. K...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  witti littl book make son laugh loud recit car...  \n",
       "1  rememb see show air televis year ago child sis...  \n",
       "2  beetlejuic well written movi everyth excel act...  \n",
       "3  twist rumplestiskin captur film star michael k...  \n",
       "4  beetlejuic excel funni movi keaton hilari wack...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_100000.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data for simple cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(20000,)\n",
      "(20000,)\n",
      "(60000,)\n",
      "(20000,)\n",
      "(20000,)\n",
      "Wall time: 3.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#storing cleanedtext into x and Score into y\n",
    "x = final_100000['CleanedText']\n",
    "y = final_100000['Score']\n",
    "#Splitting the data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "#splitting train data as train as 60% and  cross_validation as 20% and test data as 20%\n",
    "x_tr, x_cv, y_tr, y_cv = train_test_split(X_train, Y_train, test_size=0.25,shuffle=False)\n",
    "print(x_tr.shape)\n",
    "print(x_cv.shape)\n",
    "print(X_test.shape)\n",
    "print(y_tr.shape)\n",
    "print(y_cv.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data for k fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n",
      "(30000,)\n",
      "(70000,)\n",
      "(30000,)\n",
      "Wall time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#storing cleanedtext into x and Score into y\n",
    "x = final_100000['CleanedText']\n",
    "y = final_100000['Score']\n",
    "#Splitting the data into train and test data with 70:30 ratio.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=False)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.BOW:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the simple cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Bag of words:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "#vectorizing the train data,cross validate data and test data.\n",
    "bow = count_vect.fit_transform(x_tr)\n",
    "bow1 = count_vect.transform(x_cv)\n",
    "bow2 = count_vect.transform(X_test)\n",
    "#preprocessing the train data,cross validate data and test data.\n",
    "from sklearn import preprocessing\n",
    "tr_bow_x = preprocessing.normalize(bow)\n",
    "cv_bow_x = preprocessing.normalize(bow1)\n",
    "te_bow_x = preprocessing.normalize(bow2)\n",
    "savetofile(tr_bow_x,\"tr_bow_x.pickle\")\n",
    "savetofile(cv_bow_x,\"cv_bow_x.pickle\")\n",
    "savetofile(te_bow_x,\"te_bow_x.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#conversting sparse data into dense data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, n_iter=5)\n",
    "dense_tr_bow_x=svd.fit_transform(tr_bow_x)  \n",
    "dense_cv_bow_x=svd.transform(cv_bow_x)  \n",
    "dense_te_bow_x=svd.transform(te_bow_x)\n",
    "savetofile(dense_tr_bow_x,\"dense_tr_bow_x.pickle\")\n",
    "savetofile(dense_cv_bow_x,\"dense_cv_bow_x.pickle\")\n",
    "savetofile(dense_te_bow_x,\"dense_te_bow_x.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the K-fold cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Bag of words:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "#vectorizing the train data and test data.\n",
    "bow1 = count_vect.fit_transform(x_train)\n",
    "bow2 = count_vect.transform(x_test)\n",
    "#preprocessing the train data and test data.\n",
    "from sklearn import preprocessing\n",
    "kfold_tr_bow_x = preprocessing.normalize(bow1)\n",
    "kfold_te_bow_x = preprocessing.normalize(bow2)\n",
    "savetofile(kfold_tr_bow_x,\"kfold_tr_bow_x.pickle\")\n",
    "savetofile(kfold_te_bow_x,\"kfold_te_bow_x.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 300)\n",
      "(30000, 300)\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#conversting sparse data into dense data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, n_iter=5)\n",
    "dense_kfold_tr_bow_x=svd.fit_transform(kfold_tr_bow_x)\n",
    "dense_kfold_te_bow_x=svd.transform(kfold_te_bow_x)\n",
    "print(dense_kfold_tr_bow_x.shape)\n",
    "print(dense_kfold_te_bow_x.shape)\n",
    "savetofile(dense_kfold_tr_bow_x,\"dense_kfold_tr_bow_x.pickle\")\n",
    "savetofile(dense_kfold_te_bow_x,\"dense_kfold_te_bow_x.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.TF-IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the simple cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "#vectorizing the train data,cross validate data and test data.\n",
    "tfidf1 = tfidf.fit_transform(x_tr)\n",
    "tfidf2 = tfidf.transform(x_cv)\n",
    "tfidf3 = tfidf.transform(X_test)\n",
    "#preprocessing the train data,cross validate data and test data.\n",
    "from sklearn import preprocessing\n",
    "tr_tfidf_x = preprocessing.normalize(tfidf1)\n",
    "cv_tfidf_x= preprocessing.normalize(tfidf2)\n",
    "te_tfidf_x= preprocessing.normalize(tfidf3)\n",
    "savetofile(tr_tfidf_x,\"tr_tfidf_x.pickle\")\n",
    "savetofile(cv_tfidf_x,\"cv_tfidf_x.pickle\")\n",
    "savetofile(te_tfidf_x,\"te_tfidf_x.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Loading the variable from file\n",
    "tr_tfidf_x= openfromfile(\"tr_tfidf_x.pickle\")\n",
    "cv_tfidf_x= openfromfile(\"cv_tfidf_x.pickle\")\n",
    "te_tfidf_x= openfromfile(\"te_tfidf_x.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#conversting sparse data into dense data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, n_iter=5)\n",
    "dense_tr_tfidf_x =svd.fit_transform(tr_tfidf_x)\n",
    "dense_cv_tfidf_x =svd.transform(cv_tfidf_x)\n",
    "dense_te_tfidf_x =svd.transform(te_tfidf_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 300)\n",
      "(20000, 300)\n",
      "(20000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(dense_tr_tfidf_x.shape)\n",
    "print(dense_cv_tfidf_x.shape)\n",
    "print(dense_te_tfidf_x.shape)\n",
    "savetofile(dense_tr_tfidf_x,\"dense_tr_tfidf_x.pickle\")\n",
    "savetofile(dense_cv_tfidf_x,\"dense_cv_tfidf_x.pickle\")\n",
    "savetofile(dense_te_tfidf_x,\"dense_te_tfidf_x.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the K-fold cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#TFIDF:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "#vectorizing the train data and test data.\n",
    "tfidf1 = tfidf.fit_transform(x_train)\n",
    "tfidf2 = tfidf.transform(x_test)\n",
    "#preprocessing the train data test data.\n",
    "from sklearn import preprocessing\n",
    "kfold_tr_tfidf_x = preprocessing.normalize(tfidf1)\n",
    "kfold_te_tfidf_x= preprocessing.normalize(tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 421 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(kfold_tr_tfidf_x,\"kfold_tr_tfidf_x.pickle\")\n",
    "savetofile(kfold_te_tfidf_x,\"kfold_te_tfidf_x.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 300)\n",
      "(30000, 300)\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#conversting sparse data into dense data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300, n_iter=5)\n",
    "dense_kfold_tr_tfidf_x=svd.fit_transform(kfold_tr_tfidf_x)\n",
    "dense_kfold_te_tfidf_x=svd.transform(kfold_te_tfidf_x)\n",
    "print(dense_kfold_tr_tfidf_x.shape)\n",
    "print(dense_kfold_te_tfidf_x.shape)\n",
    "savetofile(dense_kfold_tr_tfidf_x,\"dense_kfold_tr_tfidf_x.pickle\")\n",
    "savetofile(dense_kfold_te_tfidf_x,\"dense_kfold_te_tfidf_x.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Avg-W2V:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the simple cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 578 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# converting train data into avg_w2vec data\n",
    "list_of_sent_tr=[]\n",
    "for sent in x_tr.values:\n",
    "    list_of_sent_tr.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9986\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training w2vec model on train data\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model_tr=gensim.models.Word2Vec(list_of_sent_tr,min_count=5,size=50, workers=4)\n",
    "words_tr = list(w2v_model_tr.wv.vocab)\n",
    "print(len(words_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "50\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding avg w2v of train data\n",
    "sent_vectors_tr = []; \n",
    "for sent in list_of_sent_tr:\n",
    "    sent_vec = np.zeros(50) \n",
    "    cnt_words =0; \n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = w2v_model_tr.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors_tr.append(sent_vec)\n",
    "print(len(sent_vectors_tr))\n",
    "print(len(sent_vectors_tr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (60000, 50)\n",
      "Wall time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of train data\n",
    "from sklearn import preprocessing\n",
    "avg_w2v_tr=preprocessing.scale(sent_vectors_tr)\n",
    "print(\"standardized data shape is\",avg_w2v_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 506 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(avg_w2v_tr,'avg_w2v_tr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#converting cv data into avg_w2vec data\n",
    "list_of_sent_cv=[]\n",
    "for sent in x_cv.values:\n",
    "    list_of_sent_cv.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "50\n",
      "Wall time: 3.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding avg w2v of cv data\n",
    "sent_vectors_cv = [];\n",
    "for sent in list_of_sent_cv: \n",
    "    sent_vec = np.zeros(50) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        try:\n",
    "            vec = w2v_model_tr.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors_cv.append(sent_vec)\n",
    "print(len(sent_vectors_cv))\n",
    "print(len(sent_vectors_cv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (20000, 50)\n",
      "Wall time: 64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of cv data\n",
    "from sklearn import preprocessing\n",
    "avg_w2v_cv=preprocessing.scale(sent_vectors_cv)\n",
    "print(\"standardized data shape is\",avg_w2v_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(avg_w2v_cv,'avg_w2v_cv.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 251 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#converting test data into avg_w2vec data\n",
    "list_of_sent_te=[]\n",
    "for sent in X_test.values:\n",
    "    list_of_sent_te.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len( list_of_sent_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "50\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding avg w2v of test data\n",
    "sent_vectors_te = [];\n",
    "for sent in list_of_sent_te: \n",
    "    sent_vec = np.zeros(50) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        try:\n",
    "            vec = w2v_model_tr.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors_te.append(sent_vec)\n",
    "print(len(sent_vectors_te))\n",
    "print(len(sent_vectors_te[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (20000, 50)\n",
      "Wall time: 61 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of test data\n",
    "from sklearn import preprocessing\n",
    "avg_w2v_te=preprocessing.scale(sent_vectors_te)\n",
    "print(\"standardized data shape is\",avg_w2v_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(avg_w2v_te,'avg_w2v_te.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the K-fold cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# converting train data into avg_w2vec data\n",
    "list_of_sent_tr_kfold=[]\n",
    "for sent in x_train.values:\n",
    "    list_of_sent_tr_kfold.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10724\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training w2vec model on train data\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model_tr_kfold=gensim.models.Word2Vec(list_of_sent_tr_kfold,min_count=5,size=50, workers=4)\n",
    "words_tr_kfold = list(w2v_model_tr_kfold.wv.vocab)\n",
    "print(len(words_tr_kfold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "50\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding avg w2v of train data\n",
    "sent_vectors_tr_kfold = []; \n",
    "for sent in list_of_sent_tr_kfold:\n",
    "    sent_vec = np.zeros(50) \n",
    "    cnt_words =0; \n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = w2v_model_tr_kfold.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors_tr_kfold.append(sent_vec)\n",
    "print(len(sent_vectors_tr_kfold))\n",
    "print(len(sent_vectors_tr_kfold[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (70000, 50)\n",
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of train data\n",
    "from sklearn import preprocessing\n",
    "kfold_avg_w2v_tr=preprocessing.scale(sent_vectors_tr_kfold)\n",
    "print(\"standardized data shape is\",kfold_avg_w2v_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 337 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(kfold_avg_w2v_tr,'kfold_avg_w2v_tr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 215 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#converting test data into avg_w2vec data\n",
    "list_of_sent_te_kfold=[]\n",
    "for sent in x_test.values:\n",
    "    list_of_sent_te_kfold.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "50\n",
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding avg w2v of test data\n",
    "sent_vectors_te_kfold = [];\n",
    "for sent in list_of_sent_te_kfold: \n",
    "    sent_vec = np.zeros(50) \n",
    "    cnt_words =0; \n",
    "    for word in sent: \n",
    "        try:\n",
    "            vec = w2v_model_tr_kfold.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors_te_kfold.append(sent_vec)\n",
    "print(len(sent_vectors_te_kfold))\n",
    "print(len(sent_vectors_te_kfold[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (30000, 50)\n",
      "Wall time: 87 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of test data\n",
    "from sklearn import preprocessing\n",
    "kfold_avg_w2v_te=preprocessing.scale(sent_vectors_te_kfold)\n",
    "print(\"standardized data shape is\",kfold_avg_w2v_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 78 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(kfold_avg_w2v_te,'kfold_avg_w2v_te.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Tf-idf- W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the simple cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tf_idf: (60000, 831429)\n"
     ]
    }
   ],
   "source": [
    "#training tfidf model on train data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf_tr = tf_idf_vect.fit_transform(x_tr.values)\n",
    "dictionary = dict(zip(tf_idf_vect.get_feature_names(), list(tf_idf_vect.idf_)))\n",
    "print(\"shape of tf_idf:\",tf_idf_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding tfidf w2v on train data\n",
    "features =tf_idf_vect.get_feature_names()\n",
    "tfidf_w2v_tr = []\n",
    "row = 0\n",
    "for sent in list_of_sent_tr:\n",
    "    sent_vec = np.zeros(50)\n",
    "    weighted_sum = 0\n",
    "    for word in sent:\n",
    "        if(word in words_tr):\n",
    "            vec = w2v_model_tr.wv[word]\n",
    "            tfidf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weighted_sum += tfidf\n",
    "    if(weighted_sum != 0):\n",
    "        sent_vec /= weighted_sum\n",
    "    tfidf_w2v_tr.append(sent_vec)      \n",
    "    row += 1\n",
    "    if row % 100 == 0:\n",
    "        print(row,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (60000, 50)\n",
      "Wall time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of test data\n",
    "from sklearn import preprocessing\n",
    "tfidf_w2v_tr=preprocessing.scale(tfidf_w2v_tr)\n",
    "print(\"standardized data shape is\",tfidf_w2v_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 317 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(tfidf_w2v_tr,'tfidf_w2v_tr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding tfidf w2v on cv data\n",
    "features =tf_idf_vect.get_feature_names()\n",
    "tfidf_w2v_cv = []\n",
    "row = 0\n",
    "for sent in list_of_sent_cv:\n",
    "    sent_vec = np.zeros(50)\n",
    "    weighted_sum = 0\n",
    "    for word in sent:\n",
    "        if(word in words_tr):\n",
    "            vec = w2v_model_tr.wv[word]\n",
    "            tfidf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weighted_sum += tfidf\n",
    "    if(weighted_sum != 0):\n",
    "        sent_vec /= weighted_sum\n",
    "    tfidf_w2v_cv.append(sent_vec)      \n",
    "    row += 1\n",
    "    if row % 100 == 0:\n",
    "        print(row,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (20000, 50)\n",
      "Wall time: 66.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of cv data\n",
    "from sklearn import preprocessing\n",
    "tfidf_w2v_cv=preprocessing.scale(tfidf_w2v_cv)\n",
    "print(\"standardized data shape is\",tfidf_w2v_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(tfidf_w2v_cv,'tfidf_w2v_cv.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding tfidf w2v on cv data\n",
    "features =tf_idf_vect.get_feature_names()\n",
    "tfidf_w2v_te = []\n",
    "row = 0\n",
    "for sent in list_of_sent_te:\n",
    "    sent_vec = np.zeros(50)\n",
    "    weighted_sum = 0\n",
    "    for word in sent:\n",
    "        if(word in words_tr):\n",
    "            vec = w2v_model_tr.wv[word]\n",
    "            tfidf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weighted_sum += tfidf\n",
    "    if(weighted_sum != 0):\n",
    "        sent_vec /= weighted_sum\n",
    "    tfidf_w2v_te.append(sent_vec)      \n",
    "    row += 1\n",
    "    if row % 100 == 0:\n",
    "        print(row,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (20000, 50)\n",
      "Wall time: 71 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of cv data\n",
    "from sklearn import preprocessing\n",
    "tfidf_w2v_te=preprocessing.scale(tfidf_w2v_te)\n",
    "print(\"standardized data shape is\",tfidf_w2v_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(tfidf_w2v_te,'tfidf_w2v_te.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorising the K-fold cross validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tf_idf: (70000, 939185)\n"
     ]
    }
   ],
   "source": [
    "#training tfidf model on train data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "kfold_tf_idf_tr = tf_idf_vect.fit_transform(x_train.values)\n",
    "dictionary = dict(zip(tf_idf_vect.get_feature_names(), list(tf_idf_vect.idf_)))\n",
    "print(\"shape of tf_idf:\",kfold_tf_idf_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding tfidf w2v on train data\n",
    "features =tf_idf_vect.get_feature_names()\n",
    "kfold_tfidf_w2v_tr = []\n",
    "row = 0\n",
    "for sent in list_of_sent_tr_kfold:\n",
    "    sent_vec = np.zeros(50)\n",
    "    weighted_sum = 0\n",
    "    for word in sent:\n",
    "        if(word in words_tr_kfold):\n",
    "            vec = w2v_model_tr_kfold.wv[word]\n",
    "            tfidf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weighted_sum += tfidf\n",
    "    if(weighted_sum != 0):\n",
    "        sent_vec /= weighted_sum\n",
    "    kfold_tfidf_w2v_tr.append(sent_vec)      \n",
    "    row += 1\n",
    "    if row % 100 == 0:\n",
    "        print(row,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (70000, 50)\n",
      "Wall time: 233 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of train data\n",
    "from sklearn import preprocessing\n",
    "kfold_tfidf_w2v_tr=preprocessing.scale(kfold_tfidf_w2v_tr)\n",
    "print(\"standardized data shape is\",kfold_tfidf_w2v_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 333 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(kfold_tfidf_w2v_tr,'kfold_tfidf_w2v_tr.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#finding tfidf w2v on test data\n",
    "features =tf_idf_vect.get_feature_names()\n",
    "kfold_tfidf_w2v_te = []\n",
    "row = 0\n",
    "for sent in list_of_sent_te_kfold:\n",
    "    sent_vec = np.zeros(50)\n",
    "    weighted_sum = 0\n",
    "    for word in sent:\n",
    "        if(word in words_tr_kfold):\n",
    "            vec = w2v_model_tr_kfold.wv[word]\n",
    "            tfidf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tfidf)\n",
    "            weighted_sum += tfidf\n",
    "    if(weighted_sum != 0):\n",
    "        sent_vec /= weighted_sum\n",
    "    kfold_tfidf_w2v_te.append(sent_vec)      \n",
    "    row += 1\n",
    "    if row % 100 == 0:\n",
    "        print(row,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized data shape is (30000, 50)\n",
      "Wall time: 98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#preprocessing avg w2v of test data\n",
    "from sklearn import preprocessing\n",
    "kfold_tfidf_w2v_te=preprocessing.scale(kfold_tfidf_w2v_te)\n",
    "print(\"standardized data shape is\",kfold_tfidf_w2v_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "savetofile(kfold_tfidf_w2v_te,'kfold_tfidf_w2v_te.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
